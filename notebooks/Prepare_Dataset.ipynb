{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ef203e",
   "metadata": {},
   "source": [
    "# Prepare labeled flow dataset from pcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4ae525d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file loaded\n",
      "idle_timeout: 10, active_timeout: 120\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd \n",
    "from nfstream import NFStreamer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    from config_manager import get_config\n",
    "    from data_processing import DataProcessor\n",
    "except ImportError:\n",
    "    if '__file__' in globals():\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        if os.path.basename(script_dir) == 'src':\n",
    "            project_root = os.path.dirname(script_dir)\n",
    "        else:\n",
    "            project_root = script_dir\n",
    "    else:\n",
    "        current_dir = os.getcwd()\n",
    "        project_root = os.path.dirname(current_dir) if current_dir.endswith('notebooks') else current_dir\n",
    "    \n",
    "    src_dir = os.path.join(project_root, 'src')\n",
    "    if src_dir not in sys.path:\n",
    "        sys.path.append(src_dir)\n",
    "    \n",
    "    from config_manager import get_config\n",
    "    from data_processing import DataProcessor\n",
    "\n",
    "if '__file__' in globals():\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    if os.path.basename(script_dir) == 'src':\n",
    "        project_root = os.path.dirname(script_dir)\n",
    "    else:\n",
    "        project_root = script_dir\n",
    "else:\n",
    "    current_dir = os.getcwd()\n",
    "    project_root = os.path.dirname(current_dir) if current_dir.endswith('notebooks') else current_dir\n",
    "\n",
    "data_dir = os.path.join(project_root, 'data')\n",
    "\n",
    "idle_timeout, active_timeout = get_config()\n",
    "print(f\"idle_timeout: {idle_timeout}, active_timeout: {active_timeout}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13db0d3",
   "metadata": {},
   "source": [
    "## Extract flows from CIC pcap files (without label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92abb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcap_to_nfstream(pcap_file, idle_timeout=10, active_timeout=120, save_csv=True):\n",
    "    \"\"\"\n",
    "    Convert a pcap file to a NFStreamer DataFrame.\n",
    "    Args:\n",
    "        pcap_file (str): Path to the pcap file.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the NFStreamer data.\n",
    "    \"\"\"\n",
    "    print(\"Converting pcap file to NFStreamer DataFrame...\")\n",
    "    # check if pcap file exists\n",
    "    if not os.path.exists(pcap_file):\n",
    "        raise FileNotFoundError(f\"The pcap file {os.path.basename(pcap_file)} does not exist.\")\n",
    "    streamer = NFStreamer(source=pcap_file, \n",
    "                             statistical_analysis=True, \n",
    "                             n_dissections=20, \n",
    "                             idle_timeout=idle_timeout,\n",
    "                             active_timeout=active_timeout\n",
    "                            )\n",
    "    print(\"New streamer instance is created\")\n",
    "    dataframe = streamer.to_pandas()\n",
    "\n",
    "    if save_csv:\n",
    "        print(\"DataFrame created from NFStreamer\")\n",
    "        # makedir if not exist\n",
    "        subdirectory = os.path.join(data_dir, 'CIC', 'nfstream')\n",
    "        if not os.path.exists(subdirectory):\n",
    "            os.makedirs(subdirectory)\n",
    "        filename = os.path.basename(pcap_file)\n",
    "        weekday = filename.split('-')[0]\n",
    "        print(f\"writing to {weekday}-WorkingHours.pcap_nfstream.csv\")\n",
    "        dataframe.to_csv(f\"{subdirectory}/{weekday}-WorkingHours.pcap_nfstream.csv\", index=False)\n",
    "        print(f\"{os.path.basename(pcap_file)} converted to NFStreamer DataFrame and saved as CSV.\")\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "495c1f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pcap file of Monday\n",
      "Converting pcap file to NFStreamer DataFrame...\n",
      "New streamer instance is created\n",
      "DataFrame created from NFStreamer\n",
      "writing to Monday-WorkingHours.pcap_nfstream.csv\n",
      "Monday-WorkingHours.pcap converted to NFStreamer DataFrame and saved as CSV.\n",
      "\n",
      "Found pcap file of Tuesday\n",
      "Converting pcap file to NFStreamer DataFrame...\n",
      "New streamer instance is created\n",
      "DataFrame created from NFStreamer\n",
      "writing to Tuesday-WorkingHours.pcap_nfstream.csv\n",
      "Tuesday-WorkingHours.pcap converted to NFStreamer DataFrame and saved as CSV.\n",
      "\n",
      "Found pcap file of Wednesday\n",
      "Converting pcap file to NFStreamer DataFrame...\n",
      "New streamer instance is created\n",
      "DataFrame created from NFStreamer\n",
      "writing to Wednesday-WorkingHours.pcap_nfstream.csv\n",
      "Wednesday-WorkingHours.pcap converted to NFStreamer DataFrame and saved as CSV.\n",
      "\n",
      "Found pcap file of Thursday\n",
      "Converting pcap file to NFStreamer DataFrame...\n",
      "New streamer instance is created\n",
      "DataFrame created from NFStreamer\n",
      "writing to Thursday-WorkingHours.pcap_nfstream.csv\n",
      "Thursday-WorkingHours.pcap converted to NFStreamer DataFrame and saved as CSV.\n",
      "\n",
      "Found pcap file of Friday\n",
      "Converting pcap file to NFStreamer DataFrame...\n",
      "New streamer instance is created\n",
      "DataFrame created from NFStreamer\n",
      "writing to Friday-WorkingHours.pcap_nfstream.csv\n",
      "Friday-WorkingHours.pcap converted to NFStreamer DataFrame and saved as CSV.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pcap_files = [\n",
    "    os.path.join(data_dir, 'CIC', 'pcap', f\"{day}-WorkingHours.pcap\")\n",
    "    for day in [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]\n",
    "]\n",
    "\n",
    "for pcap_file in pcap_files:\n",
    "    try:\n",
    "        filename = os.path.basename(pcap_file)\n",
    "        weekday = filename.split('-')[0]\n",
    "        print(f\"Found pcap file of {weekday}\")\n",
    "        pcap_to_nfstream(pcap_file, idle_timeout=idle_timeout, active_timeout=active_timeout, save_csv=True)   \n",
    "        print()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {os.path.basename(pcap_file)} not found.\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490bc19a",
   "metadata": {},
   "source": [
    "## Label CIC flows dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ec9340",
   "metadata": {},
   "source": [
    "Time Zone (Canada, Saint John): ADT (UTC-3) same as America/Halifax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13a0f3",
   "metadata": {},
   "source": [
    "### Monday 2017-07-03\n",
    "All flows labeled as  \"benign\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a1a2b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling flows in Monday-WorkingHours.pcap_nfstream.csv\n",
      "Labeling of Monday-WorkingHours.pcap_nfstream.csv is completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Labeling flows in Monday-WorkingHours.pcap_nfstream.csv\")\n",
    "\n",
    "# read Monday-WorkingHours.pcap_nfstream.csv to dataframe\n",
    "df_src_mon = pd.read_csv(os.path.join(data_dir, 'CIC', 'nfstream', \"Monday-WorkingHours.pcap_nfstream.csv\"), low_memory=False, encoding='utf-8')\n",
    "\n",
    "# add a column 'label' with value 'benign' for all rows\n",
    "df_src_mon['label'] = 'benign'\n",
    "\n",
    "# save to file Monday-WorkingHours.pcap_nfstream_labeled.csv\n",
    "df_src_mon.to_csv(os.path.join(data_dir, 'CIC', 'nfstream', \"Monday-WorkingHours.pcap_nfstream_labeled.csv\"), index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Labeling of Monday-WorkingHours.pcap_nfstream.csv is completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419def9",
   "metadata": {},
   "source": [
    "### Tuesday 2017-07-04\n",
    "\n",
    "labeled as 'ftp_patator'  \n",
    "    src_addr: 172.16.0.1  \n",
    "    dst_addr: 192.168.10.50  \n",
    "    Start: 1499170620000  \n",
    "    End: 1499175000000  \n",
    "\n",
    "labeled as 'ssh_patator'  \n",
    "    src_addr: 172.16.0.1  \n",
    "    dst_addr: 192.168.10.50  \n",
    "    Start: 1499188140000  \n",
    "    End: 1499191860000  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bc645c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling flows in Tuesday-WorkingHours.pcap_nfstream.csv\n",
      "Labeling of Tuesday-WorkingHours.pcap_nfstream.csv is completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Labeling flows in Tuesday-WorkingHours.pcap_nfstream.csv\")\n",
    "\n",
    "# read Tuesday-WorkingHours.pcap_nfstream.csv to dataframe\n",
    "df_src_tue = pd.read_csv(os.path.join(data_dir, 'CIC', 'nfstream', \"Tuesday-WorkingHours.pcap_nfstream.csv\"), low_memory=False, encoding='utf-8')\n",
    "\n",
    "# ftp_parator\n",
    "ftp_period = ((df_src_tue['bidirectional_first_seen_ms'] >= 1499170620000) &\n",
    "             (df_src_tue['bidirectional_last_seen_ms'] <= 1499175000000))\n",
    "subset_ftp = ((df_src_tue['src_ip'] == \"172.16.0.1\") & (df_src_tue['dst_ip'] == \"192.168.10.50\"))\n",
    "df_src_tue.loc[ftp_period & subset_ftp, 'label'] = 'ftp_patator'\n",
    "\n",
    "# ssh_parator\n",
    "ssh_period = ((df_src_tue['bidirectional_first_seen_ms'] >= 1499188140000) &\n",
    "             (df_src_tue['bidirectional_last_seen_ms'] <= 1499191860000))\n",
    "subset_ssh =  ((df_src_tue['src_ip'] == \"172.16.0.1\") & (df_src_tue['dst_ip'] == \"192.168.10.50\"))\n",
    "df_src_tue.loc[ ssh_period & subset_ssh, 'label'] = 'ssh_patator'\n",
    "\n",
    "# benign\n",
    "df_src_tue.loc[df_src_tue['label'].isnull(), 'label'] = 'benign'\n",
    "\n",
    "# save to file Tuesday-WorkingHours.pcap_nfstream_labeled.csv\n",
    "df_src_tue.to_csv(os.path.join(data_dir, 'CIC', 'nfstream', \"Tuesday-WorkingHours.pcap_nfstream_labeled.csv\"), index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Labeling of Tuesday-WorkingHours.pcap_nfstream.csv is completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd49b70",
   "metadata": {},
   "source": [
    "### Wednesday 2017-07-05\n",
    "\n",
    "labeled as 'dos_slowloris'  \n",
    "    src_addr: 172.16.0.1  \n",
    "    dst_addr: 192.168.10.50  \n",
    "    Start: 1499256060000  \n",
    "    End:   1499260260000  \n",
    "    Start: 1499275440000  \n",
    "    End:   1499275500000  \n",
    "  \n",
    "labeled as 'dos_slowhttptest'  \n",
    "    src_addr: 172.16.0.1  \n",
    "    dst_addr: 192.168.10.50  \n",
    "    Start: 1499260500000  \n",
    "    End: 1499261820000\n",
    "\n",
    "labeled as 'dos_hulk'  \n",
    "    src_addr: 172.16.0.1  \n",
    "    dst_addr: 192.168.10.50  \n",
    "    dst_port: 80   \n",
    "    Start: 1499262180000  \n",
    "    End: 1499263620000\n",
    "\n",
    "labeled as 'dos_goldeneye'  \n",
    "    src_addr: 172.16.0.1  \n",
    "    dst_addr: 192.168.10.50  \n",
    "    dst_port: 80  \n",
    "    Start: 1499263800000  \n",
    "    End: 1499264340000  \n",
    "\n",
    "labeled as 'heartbleed'  \n",
    "    src_addr: 172.16.0.1  \n",
    "    src_port: 45022  \n",
    "    dst_addr: 192.168.10.51  \n",
    "    dst_port:444  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3bb4426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling flows in Wednesday-WorkingHours.pcap_nfstream.csv\n",
      "Labeling of Wednesday-WorkingHours.pcap_nfstream.csv is completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Labeling flows in Wednesday-WorkingHours.pcap_nfstream.csv\")\n",
    "\n",
    "# read Wednesday-WorkingHours.pcap_nfstream.csv to dataframe\n",
    "df_src_wed = pd.read_csv(os.path.join(data_dir, 'CIC', 'nfstream', \"Wednesday-WorkingHours.pcap_nfstream.csv\"), low_memory=False, encoding='utf-8')\n",
    "\n",
    "# dos_slowloris\n",
    "slowloris_period = (((df_src_wed['bidirectional_first_seen_ms'] >= 1499256060000) & \n",
    "                     (df_src_wed['bidirectional_last_seen_ms'] <= 1499260260000)) |\n",
    "                    ((df_src_wed['bidirectional_first_seen_ms'] <= 1499275440000) &\n",
    "                     (df_src_wed['bidirectional_last_seen_ms'] <= 1499275500000)))\n",
    "subset_slowloris = ((df_src_wed['src_ip'] == \"172.16.0.1\") & (df_src_wed['dst_ip'] == \"192.168.10.50\") & (df_src_wed['dst_port'] == 80))\n",
    "df_src_wed.loc[slowloris_period & subset_slowloris, 'label'] = 'dos_slowloris'\n",
    "\n",
    "# dos_slowhttptest\n",
    "slowhttptest_period = ((df_src_wed['bidirectional_first_seen_ms'] >= 1499260500000) & \n",
    "                        (df_src_wed['bidirectional_last_seen_ms'] <= 1499261820000))\n",
    "subset_slowhttptest = ((df_src_wed['src_ip'] == \"172.16.0.1\") & (df_src_wed['dst_ip'] == \"192.168.10.50\") & (df_src_wed['dst_port'] == 80))\n",
    "df_src_wed.loc[slowhttptest_period & subset_slowhttptest, 'label'] = 'dos_slowhttptest'\n",
    "\n",
    "# dos_hulk\n",
    "hulk_period = ((df_src_wed['bidirectional_first_seen_ms'] >= 1499262180000) & \n",
    "                (df_src_wed['bidirectional_last_seen_ms'] <= 1499263620000))\n",
    "subset_hulk = ((df_src_wed['src_ip'] == \"172.16.0.1\") & (df_src_wed['dst_ip'] == \"192.168.10.50\") & (df_src_wed['dst_port'] == 80))\n",
    "df_src_wed.loc[hulk_period & subset_hulk, 'label'] = 'dos_hulk'\n",
    "\n",
    "# dos_goldeneye\n",
    "goldeneye_period = ((df_src_wed['bidirectional_first_seen_ms'] >= 1499263800000) & \n",
    "                     (df_src_wed['bidirectional_last_seen_ms'] <= 1499264340000))\n",
    "subset_goldeneye = ((df_src_wed['src_ip'] == \"172.16.0.1\") & (df_src_wed['dst_ip'] == \"192.168.10.50\") & (df_src_wed['dst_port'] == 80))\n",
    "df_src_wed.loc[goldeneye_period & subset_goldeneye, 'label'] = 'dos_goldeneye'\n",
    "\n",
    "# heartbleed\n",
    "subset_heartbleed = ((df_src_wed['src_ip'] == \"172.16.0.1\") & (df_src_wed['dst_ip'] == \"192.168.10.51\") \n",
    "                    & (df_src_wed['src_port'] == 45022) & (df_src_wed['dst_port'] == 444))\n",
    "# df_src_wed.loc[heartbleed_period & subset_heartbleed, 'label'] = 'heartbleed'\n",
    "df_src_wed.loc[subset_heartbleed, 'label'] = 'heartbleed'\n",
    "\n",
    "# otherwise: add a column 'label' with value 'benign' for all rows\n",
    "df_src_wed.loc[df_src_wed['label'].isnull(), 'label'] = 'benign'\n",
    "\n",
    "# save to file Wednesday-WorkingHours.pcap_nfstream_labeled.csv\n",
    "df_src_wed.to_csv(os.path.join(data_dir, 'CIC', 'nfstream', \"Wednesday-WorkingHours.pcap_nfstream_labeled.csv\"), index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Labeling of Wednesday-WorkingHours.pcap_nfstream.csv is completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cf6732",
   "metadata": {},
   "source": [
    "### Thursday 2017-07-06\n",
    "  \n",
    "labeled as 'webattack_bruteforce'  \n",
    "    src_addr: 172.16.0.1  \n",
    "    dst_addr: 192.168.10.50  \n",
    "    ip_prot: 6  \n",
    "    Start: 1499343300000  \n",
    "    End:   1499346000000  \n",
    "\n",
    "labeled as 'webattack_xss'  \n",
    "    src_addr: 172.16.0.1  \n",
    "    dst_addr: 192.168.10.50  \n",
    "    ip_prot: 6  \n",
    "    Start: 1499346900000\n",
    "    End: 1499348100000\n",
    "\n",
    "labeled as 'webattack_sql_injection'  \n",
    "    src_addr: 172.16.0.1  \n",
    "    dst_addr: 192.168.10.50  \n",
    "    ip_prot: 6  \n",
    "    Start: 1499348400000\n",
    "    End: 1499348520000\n",
    "\n",
    "drop all flow of Thursday afternoon  \n",
    "    'timestamp' >= 1499353200000000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae10bca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling flows in Thursday-WorkingHours.pcap_nfstream.csv\n",
      "Labeling of Thursday-WorkingHours.pcap_nfstream.csv is completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Labeling flows in Thursday-WorkingHours.pcap_nfstream.csv\")\n",
    "\n",
    "# read Thursday-WorkingHours.pcap_nfstream.csv to dataframe\n",
    "df_src_thr = pd.read_csv(os.path.join(data_dir, 'CIC', 'nfstream', \"Thursday-WorkingHours.pcap_nfstream.csv\"), low_memory=False, encoding='utf-8')\n",
    "\n",
    "# webattack_bruteforce\n",
    "bf_period = ((df_src_thr['bidirectional_first_seen_ms'] >= 1499343300000) &\n",
    "             (df_src_thr['bidirectional_last_seen_ms'] <= 1499346000000))\n",
    "subset_bf = ((df_src_thr['src_ip'] == \"172.16.0.1\") & (df_src_thr['dst_ip'] == \"192.168.10.50\") & (df_src_thr['protocol'] == 6))\n",
    "df_src_thr.loc[bf_period & subset_bf, 'label'] = 'webattack_bruteforce'\n",
    "\n",
    "# webattack_xss\n",
    "xss_period = ((df_src_thr['bidirectional_first_seen_ms'] >= 1499346900000) &\n",
    "             (df_src_thr['bidirectional_last_seen_ms'] <= 1499348100000))\n",
    "subset_xss = ((df_src_thr['src_ip'] == \"172.16.0.1\") & (df_src_thr['dst_ip'] == \"192.168.10.50\") & (df_src_thr['protocol'] == 6))\n",
    "df_src_thr.loc[xss_period & subset_xss, 'label'] = 'webattack_xss'\n",
    "\n",
    "# webattack_sql_injection\n",
    "xss_period = ((df_src_thr['bidirectional_first_seen_ms'] >= 1499348400000) &\n",
    "             (df_src_thr['bidirectional_last_seen_ms'] <= 1499348520000))\n",
    "subset_xss = ((df_src_thr['src_ip'] == \"172.16.0.1\") & (df_src_thr['dst_ip'] == \"192.168.10.50\") & (df_src_thr['protocol'] == 6))\n",
    "df_src_thr.loc[xss_period & subset_xss, 'label'] = 'webattack_sql_injection'\n",
    "\n",
    "# drop all flow of Thursday afternoon\n",
    "drop_period = (df_src_thr['bidirectional_first_seen_ms'] >= 1499353200000)\n",
    "df_src_thr.drop(df_src_thr[drop_period].index, axis=0, inplace=True)\n",
    "\n",
    "# otherwise: add a column 'label' with value 'benign' for all rows\n",
    "df_src_thr.loc[df_src_thr['label'].isnull(), 'label'] = 'benign'\n",
    "\n",
    "# save to file Thursday-WorkingHours.pcap_nfstream_labeled.csv\n",
    "df_src_thr.to_csv(os.path.join(data_dir, 'CIC', 'nfstream', \"Thursday-WorkingHours.pcap_nfstream_labeled.csv\"), index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Labeling of Thursday-WorkingHours.pcap_nfstream.csv is completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a73c029",
   "metadata": {},
   "source": [
    "### Friday 2017-07-07\n",
    "  \n",
    "drop dst_addr = \"205.174.165.73\" after 1499436193000  \n",
    "\n",
    "drop flows with ip addr 52.6.13.28 and 52.7.235.158    \n",
    "\n",
    "labeled as 'bot'  \n",
    "dst_addr: 205.174.165.73  \n",
    "Start: 1499430840000  \n",
    "End: 1499443140000  \n",
    "\n",
    "labeled as 'ddos'  \n",
    "src_addr: 172.16.0.1  \n",
    "dst_addr: 192.168.10.50  \n",
    "ip_prot: 6  \n",
    "Start: 1499453760000  \n",
    "End: 1499454960000  \n",
    "\n",
    "labeled as 'portscan'  \n",
    "src_addr: 172.16.0.1  \n",
    "dst_addr: 192.168.10.50  \n",
    "ip_prot:  6  \n",
    "Start: 1499443500000  \n",
    "End: 1499451780000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9b2e199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling flows in Friday-WorkingHours.pcap_nfstream.csv\n",
      "Dropped 1463 rows\n",
      "Dropped 26 rows\n",
      "Labeling of Friday-WorkingHours.pcap_nfstream.csv is completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Labeling flows in Friday-WorkingHours.pcap_nfstream.csv\")\n",
    "\n",
    "# read Friday-WorkingHours.pcap_nfstream.csv to dataframe\n",
    "df_src_fri = pd.read_csv(os.path.join(data_dir, 'CIC', 'nfstream', \"Friday-WorkingHours.pcap_nfstream.csv\"), low_memory=False, encoding='utf-8')\n",
    "\n",
    "# drop dst_addr = \"205.174.165.73\" after 1499436193000 \n",
    "drop_wrong_labels = ((df_src_fri['dst_ip'] == \"205.174.165.73\") & (df_src_fri['bidirectional_first_seen_ms'] > 1499436193000))\n",
    "df_src_fri.drop(df_src_fri[drop_wrong_labels].index, axis=0, inplace=True)\n",
    "# print out number of dropped rows\n",
    "print(f\"Dropped {drop_wrong_labels.sum()} rows\")\n",
    "\n",
    "# drop flows with ip addr\n",
    "drop_subset = ((df_src_fri['dst_ip'] == \"52.6.13.28\") | (df_src_fri['dst_ip']== \"52.7.235.158\"))\n",
    "df_src_fri.drop(df_src_fri[drop_subset].index, axis=0, inplace=True)\n",
    "# print out number of dropped rows\n",
    "print(f\"Dropped {drop_subset.sum()} rows\")\n",
    "\n",
    "# processing bot attacks\n",
    "bot_period = ((df_src_fri['bidirectional_first_seen_ms'] >= 1499430840000) &\n",
    "               (df_src_fri['bidirectional_last_seen_ms'] <= 1499443140000))\n",
    "subset_bot = (df_src_fri['dst_ip'] == \"205.174.165.73\")\n",
    "df_src_fri.loc[bot_period & subset_bot, 'label'] = 'bot'\n",
    "\n",
    "# ddos\n",
    "ddos_period = ((df_src_fri['bidirectional_first_seen_ms'] >= 1499453760000) &\n",
    "                (df_src_fri['bidirectional_last_seen_ms'] <= 1499454960000))\n",
    "subset_ddos = ((df_src_fri['src_ip'] == \"172.16.0.1\") & (df_src_fri['dst_ip'] == \"192.168.10.50\") & (df_src_fri['protocol'] == 6))\n",
    "df_src_fri.loc[ddos_period & subset_ddos, 'label'] = 'ddos'\n",
    "\n",
    "# portscan\n",
    "portscan_period = ((df_src_fri['bidirectional_first_seen_ms'] >= 1499443500000) &\n",
    "                    (df_src_fri['bidirectional_last_seen_ms'] <= 1499451780000))\n",
    "subset_portscan = ((df_src_fri['src_ip'] == \"172.16.0.1\") & (df_src_fri['dst_ip'] == \"192.168.10.50\") & (df_src_fri['protocol'] == 6))\n",
    "df_src_fri.loc[portscan_period & subset_portscan, 'label'] = 'portscan'\n",
    "\n",
    "# otherwise: add a column 'label' with value 'benign' for all rows\n",
    "df_src_fri.loc[df_src_fri['label'].isnull(), 'label'] = 'benign'\n",
    "\n",
    "# save to file Friday-WorkingHours.pcap_nfstream_labeled.csv\n",
    "df_src_fri.to_csv(os.path.join(data_dir, 'CIC', 'nfstream', \"Friday-WorkingHours.pcap_nfstream_labeled.csv\"), index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Labeling of Friday-WorkingHours.pcap_nfstream.csv is completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c66bcf",
   "metadata": {},
   "source": [
    "## Extract and label flows from tcpdump pcap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d6cba23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File normal_01.csv does not exist.\n",
      "normal_01.pcap -> normal_01.csv\n",
      "Converting pcap file to NFStreamer DataFrame...\n",
      "my_streamer created\n",
      "DataFrame created from NFStreamer\n",
      "normal_01.pcap converted to NFStreamer DataFrame and saved as CSV.\n",
      "Loaded 1 file with 12064 rows\n",
      "src_ip\n",
      "172.28.1.4                   12046\n",
      "172.28.1.2                      17\n",
      "fe80::58b2:3bff:feb4:cb1a        1\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "benign     12046\n",
      "unknown       18\n",
      "Name: count, dtype: int64\n",
      "Before delete: 12064\n",
      "Number of 'unknown' rows: 18\n",
      "After delete: 12046\n",
      "label\n",
      "benign    12046\n",
      "Name: count, dtype: int64\n",
      "Saved labeled file: normal_01_labeled.csv\n",
      "#rows: 12046\n",
      "#columns: 87\n",
      "\n",
      "File normal_02.csv does not exist.\n",
      "normal_02.pcap -> normal_02.csv\n",
      "Converting pcap file to NFStreamer DataFrame...\n",
      "my_streamer created\n",
      "DataFrame created from NFStreamer\n",
      "normal_02.pcap converted to NFStreamer DataFrame and saved as CSV.\n",
      "Loaded 1 file with 24121 rows\n",
      "src_ip\n",
      "172.28.1.4                   24092\n",
      "172.28.1.2                      27\n",
      "fe80::c0f4:f5ff:fe96:d40         1\n",
      "fe80::dc2c:16ff:fe7a:1b90        1\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "benign     24092\n",
      "unknown       29\n",
      "Name: count, dtype: int64\n",
      "Before delete: 24121\n",
      "Number of 'unknown' rows: 29\n",
      "After delete: 24092\n",
      "label\n",
      "benign    24092\n",
      "Name: count, dtype: int64\n",
      "Saved labeled file: normal_02_labeled.csv\n",
      "#rows: 24092\n",
      "#columns: 87\n",
      "\n",
      "File normal_and_attack_01.csv does not exist.\n",
      "normal_and_attack_01.pcap -> normal_and_attack_01.csv\n",
      "Converting pcap file to NFStreamer DataFrame...\n",
      "my_streamer created\n",
      "DataFrame created from NFStreamer\n",
      "normal_and_attack_01.pcap converted to NFStreamer DataFrame and saved as CSV.\n",
      "Loaded 1 file with 7748 rows\n",
      "src_ip\n",
      "172.28.1.3                   5708\n",
      "172.28.1.4                   2032\n",
      "172.28.1.2                      7\n",
      "fe80::88a8:cfff:fe8c:a8b5       1\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "anomalous    5708\n",
      "benign       2032\n",
      "unknown         8\n",
      "Name: count, dtype: int64\n",
      "Before delete: 7748\n",
      "Number of 'unknown' rows: 8\n",
      "After delete: 7740\n",
      "label\n",
      "anomalous    5708\n",
      "benign       2032\n",
      "Name: count, dtype: int64\n",
      "Saved labeled file: normal_and_attack_01_labeled.csv\n",
      "#rows: 7740\n",
      "#columns: 87\n",
      "\n",
      "File normal_and_attack_02.csv does not exist.\n",
      "normal_and_attack_02.pcap -> normal_and_attack_02.csv\n",
      "Converting pcap file to NFStreamer DataFrame...\n",
      "my_streamer created\n",
      "DataFrame created from NFStreamer\n",
      "normal_and_attack_02.pcap converted to NFStreamer DataFrame and saved as CSV.\n",
      "Loaded 1 file with 6010 rows\n",
      "src_ip\n",
      "172.28.1.3                 4036\n",
      "172.28.1.4                 1966\n",
      "172.28.1.2                    7\n",
      "fe80::c7b:fff:fee7:7284       1\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "anomalous    4036\n",
      "benign       1966\n",
      "unknown         8\n",
      "Name: count, dtype: int64\n",
      "Before delete: 6010\n",
      "Number of 'unknown' rows: 8\n",
      "After delete: 6002\n",
      "label\n",
      "anomalous    4036\n",
      "benign       1966\n",
      "Name: count, dtype: int64\n",
      "Saved labeled file: normal_and_attack_02_labeled.csv\n",
      "#rows: 6002\n",
      "#columns: 87\n",
      "\n",
      "File normal_and_attack_03.csv does not exist.\n",
      "normal_and_attack_03.pcap -> normal_and_attack_03.csv\n",
      "Converting pcap file to NFStreamer DataFrame...\n",
      "my_streamer created\n",
      "DataFrame created from NFStreamer\n",
      "normal_and_attack_03.pcap converted to NFStreamer DataFrame and saved as CSV.\n",
      "Loaded 1 file with 6779 rows\n",
      "src_ip\n",
      "172.28.1.3                   4763\n",
      "172.28.1.4                   2008\n",
      "172.28.1.2                      7\n",
      "fe80::dc8d:bbff:fe56:baa3       1\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "anomalous    4763\n",
      "benign       2008\n",
      "unknown         8\n",
      "Name: count, dtype: int64\n",
      "Before delete: 6779\n",
      "Number of 'unknown' rows: 8\n",
      "After delete: 6771\n",
      "label\n",
      "anomalous    4763\n",
      "benign       2008\n",
      "Name: count, dtype: int64\n",
      "Saved labeled file: normal_and_attack_03_labeled.csv\n",
      "#rows: 6771\n",
      "#columns: 87\n",
      "\n",
      "File normal_and_attack_04.csv does not exist.\n",
      "normal_and_attack_04.pcap -> normal_and_attack_04.csv\n",
      "Converting pcap file to NFStreamer DataFrame...\n",
      "my_streamer created\n",
      "DataFrame created from NFStreamer\n",
      "normal_and_attack_04.pcap converted to NFStreamer DataFrame and saved as CSV.\n",
      "Loaded 1 file with 5700 rows\n",
      "src_ip\n",
      "172.28.1.3                 3744\n",
      "172.28.1.4                 1948\n",
      "172.28.1.2                    7\n",
      "fe80::4fc:acff:fe27:ad1       1\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "anomalous    3744\n",
      "benign       1948\n",
      "unknown         8\n",
      "Name: count, dtype: int64\n",
      "Before delete: 5700\n",
      "Number of 'unknown' rows: 8\n",
      "After delete: 5692\n",
      "label\n",
      "anomalous    3744\n",
      "benign       1948\n",
      "Name: count, dtype: int64\n",
      "Saved labeled file: normal_and_attack_04_labeled.csv\n",
      "#rows: 5692\n",
      "#columns: 87\n",
      "\n",
      "File normal_and_attack_05.csv does not exist.\n",
      "normal_and_attack_05.pcap -> normal_and_attack_05.csv\n",
      "Converting pcap file to NFStreamer DataFrame...\n",
      "my_streamer created\n",
      "DataFrame created from NFStreamer\n",
      "normal_and_attack_05.pcap converted to NFStreamer DataFrame and saved as CSV.\n",
      "Loaded 1 file with 10845 rows\n",
      "src_ip\n",
      "172.28.1.3                  8859\n",
      "172.28.1.4                  1978\n",
      "172.28.1.2                     7\n",
      "fe80::c04:c2ff:fe64:ac75       1\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "anomalous    8859\n",
      "benign       1978\n",
      "unknown         8\n",
      "Name: count, dtype: int64\n",
      "Before delete: 10845\n",
      "Number of 'unknown' rows: 8\n",
      "After delete: 10837\n",
      "label\n",
      "anomalous    8859\n",
      "benign       1978\n",
      "Name: count, dtype: int64\n",
      "Saved labeled file: normal_and_attack_05_labeled.csv\n",
      "#rows: 10837\n",
      "#columns: 87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pcap_dir = os.path.join(data_dir, 'tcpdump', 'pcap')\n",
    "# list of pcap files in pcap_dir, only filenames withaout extension\n",
    "filenames = [f[:-5] for f in os.listdir(pcap_dir) if f.endswith('.pcap')]\n",
    "filenames.sort()\n",
    "\n",
    "for filename in filenames:\n",
    "    pcap_file = os.path.join(pcap_dir, f\"{filename}.pcap\")\n",
    "    csv_file = os.path.join(data_dir, 'tcpdump', 'nfstream', f\"{filename}.csv\")\n",
    "    csv_labeled_file = os.path.join(data_dir, 'tcpdump', 'nfstream', f\"{filename}_labeled.csv\")\n",
    "\n",
    "    if os.path.exists(csv_labeled_file):\n",
    "        print(f\"File {filename}_labeled.csv already exists\")\n",
    "        df = pd.read_csv(csv_labeled_file, low_memory=False)\n",
    "        print(df['label'].value_counts())\n",
    "        print()\n",
    "        continue\n",
    "\n",
    "    if not os.path.exists(pcap_file):\n",
    "        print(f\"File {filename}.pcap does not exist.\")\n",
    "        print()\n",
    "        continue\n",
    "    \n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"File {filename}.csv does not exist.\")\n",
    "        print(f\"{os.path.basename(pcap_file)} -> {os.path.basename(csv_file)}\")\n",
    "        df = DataProcessor.pcap2nfstream(pcap_file, csv_file, idle_timeout=idle_timeout, active_timeout=active_timeout, save_csv=True)\n",
    "    else:\n",
    "        print(f\"File {filename}.csv already exists, loading from CSV.\")\n",
    "        df = pd.read_csv(csv_file, low_memory=False)\n",
    "        \n",
    "    print(df[\"src_ip\"].value_counts())\n",
    "\n",
    "    ip_anomalous = '172.28.1.3'\n",
    "    ip_benign = '172.28.1.4'\n",
    "\n",
    "    df['label'] = 'unknown' \n",
    "    df.loc[df['src_ip'] == ip_anomalous, 'label'] = 'anomalous'\n",
    "    df.loc[df['src_ip'] == ip_benign, 'label'] = 'benign'\n",
    "    print(df['label'].value_counts())\n",
    "\n",
    "    unknown_count = df[df['label'] == 'unknown'].shape[0]\n",
    "    print(f\"Before delete: {df.shape[0]}\")\n",
    "    print(f\"Number of 'unknown' rows: {unknown_count}\")\n",
    "\n",
    "    # delete 'unknown' rows\n",
    "    df = df[df['label'] != 'unknown']\n",
    "    print(f\"After delete: {df.shape[0]}\")\n",
    "    print(df['label'].value_counts())\n",
    "\n",
    "    df.to_csv(csv_labeled_file, index=False)\n",
    "    print(f\"Saved labeled file: {os.path.basename(csv_labeled_file)}\")\n",
    "\n",
    "    print(f\"#rows: {df.shape[0]}\")\n",
    "    print(f\"#columns: {df.shape[1]}\")\n",
    "\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
